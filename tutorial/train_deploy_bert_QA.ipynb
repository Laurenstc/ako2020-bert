{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on: Training and deploying NLP question answering models on AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will focus on adapting the BERT model for the question answering task on the SQuAD dataset. Specifically, we will:\n",
    "\n",
    "- understand how to pre-process the SQuAD dataset to leverage the learnt representation in BERT,\n",
    "- adapt the BERT model to the question answering task, and\n",
    "- load a trained model to perform inference on the SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras-mxnet                        2.2.4.2       \n",
      "mxnet-cu101                        1.6.0b20191122\n",
      "mxnet-model-server                 1.0.5         \n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "gluonnlp                           0.9.0.dev0    \n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "hdijupyterutils                    0.13.1        \n",
      "jupyter                            1.0.0         \n",
      "jupyter-client                     5.2.3         \n",
      "jupyter-console                    5.2.0         \n",
      "jupyter-core                       4.4.0         \n",
      "jupyterlab                         0.32.1        \n",
      "jupyterlab-launcher                0.10.5        \n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# this notebook requires mxnet-cu101 >= 1.6.0b20191102, gluonnlp >= 0.8.1\n",
    "# you can create a sagemaker notebook instance with the lifecycle configuration file: sagemaker-lifecycle.config\n",
    "!pip list | grep mxnet\n",
    "!pip list | grep gluonnlp\n",
    "!pip list | grep jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: mxnet-cu101 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (1.6.0b20191122)\n",
      "Requirement not upgraded as not directly required: graphviz<0.9.0,>=0.8.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from mxnet-cu101) (0.8.4)\n",
      "Requirement not upgraded as not directly required: numpy<2.0.0,>1.16.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from mxnet-cu101) (1.16.4)\n",
      "Requirement not upgraded as not directly required: requests<3,>=2.20.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from mxnet-cu101) (2.20.1)\n",
      "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
      "Requirement not upgraded as not directly required: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.6)\n",
      "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (2019.11.28)\n",
      "Requirement not upgraded as not directly required: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.23)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet-cu101 --pre -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras-mxnet                        2.2.4.2       \n",
      "mxnet-cu101                        1.6.0b20191122\n",
      "mxnet-model-server                 1.0.5         \n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "gluonnlp                           0.9.0.dev0    \n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "hdijupyterutils                    0.13.1        \n",
      "jupyter                            1.0.0         \n",
      "jupyter-client                     5.2.3         \n",
      "jupyter-console                    5.2.0         \n",
      "jupyter-core                       4.4.0         \n",
      "jupyterlab                         0.32.1        \n",
      "jupyterlab-launcher                0.10.5        \n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# this notebook requires mxnet-cu101 >= 1.6.0b20191102, gluonnlp >= 0.8.1\n",
    "# you can create a sagemaker notebook instance with the lifecycle configuration file: sagemaker-lifecycle.config\n",
    "!pip list | grep mxnet\n",
    "!pip list | grep gluonnlp\n",
    "!pip list | grep jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import argparse, collections, time, logging\n",
    "import numpy as np\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "# import bert\n",
    "import qa_utils\n",
    "from bert.bert_qa_evaluate import PredResult, predict\n",
    "\n",
    "# Hyperparameters\n",
    "parser = argparse.ArgumentParser('BERT finetuning')\n",
    "parser.add_argument('--batch_size', default=32)\n",
    "parser.add_argument('--num_epochs', default=1)\n",
    "parser.add_argument('--lr', default=5e-5)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "batch_size = args.batch_size\n",
    "num_epochs = args.num_epochs\n",
    "lr = args.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Get Pre-trained BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can load the pre-trained BERT easily using the model API in GluonNLP, which returns the vocabulary along with the model. We include the pooler layer of the pre-trained model by setting `use_pooler` to `True`.\n",
    "The list of pre-trained BERT models available in GluonNLP can be found [here](http://gluon-nlp.mxnet.io/model_zoo/bert/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:54.906532Z",
     "start_time": "2019-07-26T22:44:34.102308Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ctx = mx.gpu(0)\n",
    "\n",
    "## multi-gpu training\n",
    "GPU_COUNT = 4 # increase if you have more\n",
    "ctx = [mx.gpu(i) for i in range(GPU_COUNT)]\n",
    "\n",
    "bert_model, vocabulary = nlp.model.get_model('bert_12_768_12', # the 12-layer BERT Base model\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        # use pre-trained weights\n",
    "                                        pretrained=True, ctx=ctx,\n",
    "                                        # decoder and classifier are for pre-training only\n",
    "                                        use_decoder=False, use_classifier=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that we have loaded the BERT model, we only need to attach an additional layer for classification.\n",
    "The `BERTClassifier` class uses a BERT base model to encode sentence representation, followed by a `nn.Dense` layer for classification. We only need to initialize the classification layer. The encoding layers are already initialized with pre-trained weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "net = nlp.model.BERTClassifier(bert_model, num_classes=2)\n",
    "net.classifier.initialize(ctx=ctx)  # only initialize the classification layer from scratch\n",
    "net.hybridize()  # compile the model, required for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To use the pre-trained BERT model, we need to:\n",
    "- tokenize the inputs into words,\n",
    "- insert [CLS] at the beginning of a sentence, \n",
    "- insert [SEP] at the end of a sentence, and\n",
    "- generate segment ids\n",
    "\n",
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We again use the IMDB dataset, but for this time, downloading using the GluonNLP data API. We then use the transform API to transform the raw scores to positive labels and negative labels. \n",
    "To process sentences with BERT-style '[CLS]', '[SEP]' tokens, you can use `data.BERTSentenceTransform` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:57.188028Z",
     "start_time": "2019-07-26T22:44:57.181395Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_raw = nlp.data.IMDB('train')\n",
    "test_dataset_raw = nlp.data.IMDB('test')\n",
    "\n",
    "# tokenize texts into words\n",
    "tokenizer = nlp.data.BERTTokenizer(vocabulary)\n",
    "# add begin-of-sentence, end-of-sentence tokens and perform vocabulary lookup\n",
    "transform = nlp.data.BERTSentenceTransform(tokenizer, max_seq_length=128, pair=False, pad=False)\n",
    "\n",
    "def transform_fn(data):\n",
    "    # transform texts to tensors\n",
    "    text, label = data\n",
    "    # transform label into position / negative\n",
    "    label = 1 if label >= 5 else 0\n",
    "    data, length, segment_type = transform([text])\n",
    "    return data.astype('float32'), length.astype('float32'), segment_type.astype('float32'), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:57.197310Z",
     "start_time": "2019-07-26T22:44:57.189448Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence = \n",
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "\n",
      "word indices = \n",
      "[    2 22953  2213  4381  2152  2003  1037  9476  4038  1012  2009  2743\n",
      "  2012  1996  2168  2051  2004  2070  2060  3454  2055  2082  2166  1010\n",
      "  2107  2004  1000  5089  1000  1012  2026  3486  2086  1999  1996  4252\n",
      "  9518  2599  2033  2000  2903  2008 22953  2213  4381  2152  1005  1055\n",
      " 18312  2003  2172  3553  2000  4507  2084  2003  1000  5089  1000  1012\n",
      "  1996 25740  2000  5788 13732  1010  1996 12369  3993  2493  2040  2064\n",
      "  2156  2157  2083  2037 17203  5089  1005 13433  8737  1010  1996  9004\n",
      " 10196  4757  1997  1996  2878  3663  1010  2035 10825  2033  1997  1996\n",
      "  2816  1045  2354  1998  2037  2493  1012  2043  1045  2387  1996  2792\n",
      "  1999  2029  1037  3076  8385  2699  2000  6402  2091  1996  2082  1010\n",
      "  1045  3202  7383  1012  1012  1012  1012     3]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset_raw.transform(transform_fn)\n",
    "test_dataset = test_dataset_raw.transform(transform_fn)\n",
    "\n",
    "data, length, _, label = train_dataset[0]\n",
    "print('original sentence = \\n{}'.format(train_dataset_raw[0][0]))\n",
    "print('\\nword indices = \\n{}'.format(data.astype('int32')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we have all the pieces to put together, and we can finally start fine-tuning the\n",
    "model with a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "padding_id = vocabulary[vocabulary.padding_token]\n",
    "batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0, pad_val=padding_id), # words\n",
    "        nlp.data.batchify.Stack(), # valid length\n",
    "        nlp.data.batchify.Pad(axis=0, pad_val=0), # segment type\n",
    "        nlp.data.batchify.Stack(np.float32)) # label\n",
    "\n",
    "train_data = mx.gluon.data.DataLoader(train_dataset,\n",
    "                               batchify_fn=batchify_fn, shuffle=True,\n",
    "                               batch_size=batch_size, num_workers=4)\n",
    "test_data = mx.gluon.data.DataLoader(test_dataset,\n",
    "                              batchify_fn=batchify_fn,\n",
    "                              shuffle=False, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon.contrib.estimator import TrainBegin, BatchBegin, LoggingHandler\n",
    "\n",
    "\n",
    "class MyLearningRateHandler(TrainBegin, BatchBegin):\n",
    "    \"\"\"Warm-up learning rate handler.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trainer: gluon.Trainer\n",
    "        Trainer object to adjust the learning rate on.\n",
    "    num_warmup_steps: int\n",
    "        Number of initial steps during which the learning rate is linearly\n",
    "        increased to it's target.\n",
    "    num_train_steps: int\n",
    "        Total number of steps to be taken during training. Should be equal to\n",
    "        the number of batches * number of epochs.\n",
    "    lr: float\n",
    "        Base learning rate to reach after warmup.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trainer, num_warmup_steps, num_train_steps, lr):\n",
    "        self.trainer = trainer\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.lr = lr\n",
    "\n",
    "        self.step_num = 0\n",
    "\n",
    "    def train_begin(self, estimator, *args, **kwargs):\n",
    "        self.step_num = 0\n",
    "\n",
    "    def batch_begin(self, estimator, *args, **kwargs):\n",
    "        self.step_num += 1\n",
    "        if self.step_num < self.num_warmup_steps:\n",
    "            new_lr = self.lr * self.step_num / self.num_warmup_steps\n",
    "        else:\n",
    "            non_warmup_steps = self.step_num - self.num_warmup_steps\n",
    "            offset = non_warmup_steps / (self.num_train_steps - self.num_warmup_steps)\n",
    "            new_lr = self.lr - offset * self.lr\n",
    "        self.trainer.set_learning_rate(new_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon.contrib import estimator\n",
    "from mxnet.gluon.utils import split_and_load\n",
    "\n",
    "class MyEstimator(estimator.Estimator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # params for grad clipping\n",
    "        self.params = [p for p in self.net.collect_params().values() if p.grad_req != 'null']\n",
    "        \n",
    "    def fit_batch(self, train_batch, batch_axis=0):\n",
    "        train_batch = [split_and_load(x, ctx_list=self.context, batch_axis=batch_axis) for x in train_batch]\n",
    "        with mx.autograd.record():\n",
    "            pred = [self.net(inp, token_type, seq_len) for inp, seq_len, token_type, _ in zip(*train_batch)]\n",
    "            loss = [self.loss(out, label.astype('float32')) for out, _, _, _, label in zip(pred, *train_batch)]\n",
    "        mx.autograd.backward(loss)\n",
    "\n",
    "        # Gradient clipping\n",
    "        trainer.allreduce_grads()\n",
    "        nlp.utils.clip_grad_global_norm(self.params, 1)\n",
    "        trainer.update(1)\n",
    "\n",
    "        return train_batch[:3], train_batch[3], pred, loss\n",
    "\n",
    "    def evaluate_batch(self, val_batch, val_metrics, batch_axis=0):\n",
    "        val_batch = [split_and_load(x, ctx_list=self.context, batch_axis=batch_axis) for x in val_batch]\n",
    "        pred = [self.net(inp, token_type, seq_len) for inp, seq_len, token_type, _ in zip(*val_batch)]\n",
    "        label = [l for _, _, _, l in zip(*val_batch)]\n",
    "        # update metrics\n",
    "        for metric in val_metrics:\n",
    "            metric.update(label, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begin: using optimizer BERTAdam with current learning rate 0.0001 \n",
      "Train for 1 epochs.\n",
      "[Epoch 0] Begin, current learning rate: 0.0001\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "allreduce_grads() when parameters are updated on kvstore is not supported. Try setting `update_on_kvstore` to False when creating trainer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8c94fc605cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_handlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevent_handlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/contrib/estimator/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, val_data, epochs, event_handlers, batches, batch_axis)\u001b[0m\n\u001b[1;32m    377\u001b[0m                     \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;31m# batch end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5cad7aec1bd8>\u001b[0m in \u001b[0;36mfit_batch\u001b[0;34m(self, train_batch, batch_axis)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallreduce_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_global_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/trainer.py\u001b[0m in \u001b[0;36mallreduce_grads\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kvstore\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_on_kvstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                 \u001b[0;34m'allreduce_grads() when parameters are updated on kvstore '\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m                 \u001b[0;34m'is not supported. Try setting `update_on_kvstore` '\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;34m'to False when creating trainer.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: allreduce_grads() when parameters are updated on kvstore is not supported. Try setting `update_on_kvstore` to False when creating trainer."
     ]
    }
   ],
   "source": [
    "trainer = mx.gluon.Trainer(net.collect_params(), 'bertadam',\n",
    "                        {'learning_rate': lr, 'wd':0.01})\n",
    "loss_fn = mx.gluon.loss.SoftmaxCELoss()\n",
    "metrics = [mx.metric.Loss(), mx.metric.Accuracy()]\n",
    "lr_handler = MyLearningRateHandler(trainer=trainer, num_warmup_steps=50, lr=5e-5,\n",
    "                                   num_train_steps = len(train_data) * num_epochs)\n",
    "logging_handler = LoggingHandler(train_metrics=metrics, verbose=LoggingHandler.LOG_PER_BATCH)\n",
    "event_handlers = [lr_handler, logging_handler]\n",
    "\n",
    "est = MyEstimator(net=net, loss=loss_fn, metrics=metrics, trainer=trainer, context=ctx)\n",
    "est.fit(train_data=train_data, epochs=num_epochs, event_handlers=event_handlers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Validation and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "val_metric = mx.metric.Accuracy()\n",
    "est.evaluate(test_data, val_metrics=[val_metric])\n",
    "print('Validation {} = {}'.format(*val_metric.get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(net, ctx, transform, sentence):\n",
    "    ctx = ctx[0] if isinstance(ctx, list) else ctx\n",
    "    inputs, seq_len, token_types = transform([sentence])\n",
    "    inputs = mx.nd.array([inputs], ctx=ctx)\n",
    "    token_types = mx.nd.array([token_types], ctx=ctx)\n",
    "    seq_len = mx.nd.array([seq_len], ctx=ctx)\n",
    "    out = net(inputs, token_types, seq_len)\n",
    "    label = mx.nd.argmax(out, axis=1)\n",
    "    return 'positive' if label.asscalar() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "predict_sentiment(net, ctx, transform, 'this movie is so great')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deploy on SageMaker\n",
    "\n",
    "1. Model parameters\n",
    "2. Code with data pre-processing and model inference\n",
    "3. A docker container with dependencies installed\n",
    "4. Launch a serving end-point with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Save Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# save parameters, model definition and vocabulary in a zip file\n",
    "net.export('checkpoint')\n",
    "with open('vocab.json', 'w') as f:\n",
    "    f.write(vocabulary.to_json())\n",
    "import tarfile\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"checkpoint-0000.params\") \n",
    "    tar.add(\"checkpoint-symbol.json\") \n",
    "    tar.add(\"vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. the Code for Inference\n",
    "\n",
    "Two functions: \n",
    "1. model_fn() to load model parameters\n",
    "2. transform_fn() to run model inference given an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile serve.py\n",
    "import json, logging, warnings\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the gluon model. Called once when hosting service starts.\n",
    "    :param: model_dir The directory where model files are stored.\n",
    "    :return: a Gluon model, and the vocabulary\n",
    "    \"\"\"\n",
    "    prefix = 'checkpoint'\n",
    "    net = mx.gluon.nn.SymbolBlock.imports(prefix + '-symbol.json',\n",
    "                                          ['data0', 'data1', 'data2'],\n",
    "                                          prefix + '-0000.params')\n",
    "    net.load_parameters('%s/' % model_dir + prefix + '-0000.params',\n",
    "                        ctx=mx.cpu())\n",
    "    vocab_json = open('%s/vocab.json' % model_dir).read()\n",
    "    vocab = nlp.Vocab.from_json(vocab_json)\n",
    "    tokenizer = nlp.data.BERTTokenizer(vocab)\n",
    "    transform = nlp.data.BERTSentenceTransform(tokenizer, max_seq_length=128,\n",
    "                                               pair=False, pad=False)\n",
    "    return net, vocab, transform\n",
    "\n",
    "\n",
    "def transform_fn(model, data, input_content_type, output_content_type):\n",
    "    \"\"\"\n",
    "    Transform a request using the Gluon model. Called once per request.\n",
    "    :param model: The Gluon model and the vocab\n",
    "    :param data: The request payload.\n",
    "    :param input_content_type: The request content type.\n",
    "    :param output_content_type: The (desired) response content type.\n",
    "    :return: response payload and content type.\n",
    "    \"\"\"\n",
    "    # we can use content types to vary input/output handling, but\n",
    "    # here we just assume json for both\n",
    "    net, vocabulary, transform = model\n",
    "    sentence = json.loads(data)\n",
    "    result = predict_sentiment(net, mx.cpu(), transform, sentence)\n",
    "    response_body = json.dumps(result)\n",
    "    return response_body, output_content_type\n",
    "\n",
    "\n",
    "def predict_sentiment(net, ctx, transform, sentence):\n",
    "    ctx = ctx[0] if isinstance(ctx, list) else ctx\n",
    "    inputs, seq_len, token_types = transform([sentence])\n",
    "    inputs = mx.nd.array([inputs], ctx=ctx)\n",
    "    token_types = mx.nd.array([token_types], ctx=ctx)\n",
    "    seq_len = mx.nd.array([seq_len], ctx=ctx)\n",
    "    out = net(inputs, token_types, seq_len)\n",
    "    label = mx.nd.argmax(out, axis=1)\n",
    "    return 'positive' if label.asscalar() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Build a Docker Container for Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's prepare a docker container with all the dependencies required for model inference. Here we build a docker container based on the SageMaker MXNet inference container, and you can find the list of all available inference containers at https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html\n",
    "\n",
    "Here we use local mode for demonstration purpose. To deploy on actual instances, you need to login into AWS elastic container registry (ECR) service, and push the container to ECR. \n",
    "\n",
    "```\n",
    "docker build -t $YOUR_EDR_DOCKER_TAG . -f Dockerfile\n",
    "$(aws ecr get-login --no-include-email --region $YOUR_REGION)\n",
    "docker push $YOUR_EDR_DOCKER_TAG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "ARG REGION\n",
    "FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/mxnet-inference:1.4.1-gpu-py3\n",
    "\n",
    "RUN pip install --upgrade --user --pre 'mxnet-mkl' 'https://github.com/dmlc/gluon-nlp/tarball/v0.9.x'\n",
    "\n",
    "RUN pip list | grep mxnet\n",
    "\n",
    "COPY *.py /opt/ml/model/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!export REGION=$(wget -qO- http://169.254.169.254/latest/meta-data/placement/availability-zone) &&\\\n",
    " docker build --no-cache --build-arg REGION=${REGION::-1} -t my-docker:inference . -f Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4. Use SageMaker SDK to Deploy the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We create a MXNet model which can be deployed later, by specifying the docker image, and entry point for the inference code. If serve.py does not work, use dummy_hosting_module.py for debugging purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "sagemaker_model = MXNetModel(model_data='file:///home/ec2-user/SageMaker/reinvent19-gluonnlp/tutorial/model.tar.gz',\n",
    "                             image='my-docker:inference', # docker images\n",
    "                             role=sagemaker.get_execution_role(), \n",
    "                             py_version='py3',            # python version\n",
    "                             entry_point='serve.py',\n",
    "                             source_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We use 'local' mode to test our deployment code, where the inference happens on the current instance.\n",
    "If you are ready to deploy the model on a new instance, change the `instance_type` argument to values such as `ml.c4.xlarge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Here we use 'local' mode for testing, for real instances use c5.2xlarge, p2.xlarge, etc\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "output = predictor.predict('The model is deployed. Great!')  \n",
    "print('\\nPrediction output: {}\\n\\n'.format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clean Up\n",
    "\n",
    "Remove the endpoint after we are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Resources\n",
    "- Amazon SageMaker https://aws.amazon.com/sagemaker/\n",
    "- Amazon SageMaker Python SDK https://sagemaker.readthedocs.io/\n",
    "- GluonNLP http://gluon-nlp.mxnet.io/\n",
    "- GluonCV http://gluon-cv.mxnet.io/\n",
    "- GluonTS https://gluon-ts.mxnet.io/\n",
    "- Dive into Deep Learning http://d2l.ai/\n",
    "- MXNet Forum https://discuss.mxnet.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For more fine-tuning scripts, visit the [BERT model zoo webpage](http://gluon-nlp.mxnet.io/model_zoo/bert/index.html).\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Devlin, Jacob, et al. \"Bert:\n",
    "Pre-training of deep\n",
    "bidirectional transformers for language understanding.\"\n",
    "arXiv preprint\n",
    "arXiv:1810.04805 (2018).\n",
    "\n",
    "[2] Dolan, William B., and Chris\n",
    "Brockett.\n",
    "\"Automatically constructing a corpus of sentential paraphrases.\"\n",
    "Proceedings of\n",
    "the Third International Workshop on Paraphrasing (IWP2005). 2005.\n",
    "\n",
    "[3] Peters,\n",
    "Matthew E., et al. \"Deep contextualized word representations.\" arXiv\n",
    "preprint\n",
    "arXiv:1802.05365 (2018).\n",
    "\n",
    "[4] Hendrycks, Dan, and Kevin Gimpel. \"Gaussian error linear units (gelus).\" arXiv preprint arXiv:1606.08415 (2016)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
