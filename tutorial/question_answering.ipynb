{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on: Training and deploying Question Answering with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained language representations have been shown to improve many downstream NLP tasks such as question answering, and natural language inference. Devlin, Jacob, et al proposed BERT [1] (Bidirectional Encoder Representations from Transformers), which fine-tunes deep bidirectional representations on a wide range of tasks with minimal task-specific parameters, and obtained state- of-the-art results.\n",
    "\n",
    "In this tutorial, we will focus on adapting the BERT model for the question answering task on the SQuAD dataset. Specifically, we will:\n",
    "\n",
    "- understand how to pre-process the SQuAD dataset to leverage the learnt representation in BERT,\n",
    "- adapt the BERT model to the question answering task, and\n",
    "- load a trained model to perform inference on the SQuAD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker configuration\n",
    "\n",
    "This notebook requires mxnet-cu101 >= 1.6.0b20191102, gluonnlp >= 0.8.1\n",
    "We can create a sagemaker notebook instance with the lifecycle configuration file: sagemaker-lifecycle.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One time script\n",
    "# !bash sagemaker-lifecycle.config && pip install mxnet-cu101 --pre -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras-mxnet                        2.2.4.2       \n",
      "mxnet-cu101                        1.6.0b20191122\n",
      "mxnet-model-server                 1.0.5         \n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "gluonnlp                           0.9.0.dev0    \n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep mxnet\n",
    "!pip list | grep gluonnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Load MXNet and GluonNLP\n",
    "\n",
    "We first import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, collections, time, logging\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "import bert\n",
    "import qa_utils\n",
    "\n",
    "from gluonnlp.data import SQuAD\n",
    "from bert.model.qa import BertForQALoss, BertForQA\n",
    "from bert.data.qa import SQuADTransform, preprocess_dataset\n",
    "from bert.bert_qa_evaluate import get_F1_EM, predict, PredResult\n",
    "\n",
    "# Hyperparameters\n",
    "parser = argparse.ArgumentParser('BERT finetuning')\n",
    "parser.add_argument('--epochs', type=int, default=3)\n",
    "parser.add_argument('--batch_size', default=32)\n",
    "parser.add_argument('--num_epochs', default=1)\n",
    "parser.add_argument('--lr', default=5e-5)\n",
    "\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "num_epochs = args.num_epochs\n",
    "lr = args.lr\n",
    "\n",
    "# output_dir = args.output_dir\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.mkdir(output_dir)\n",
    "# test_batch_size = args.test_batch_size\n",
    "# optimizer = args.optimizer\n",
    "# accumulate = args.accumulate\n",
    "# warmup_ratio = args.warmup_ratio\n",
    "# log_interval = args.log_interval\n",
    "# max_seq_length = args.max_seq_length\n",
    "# doc_stride = args.doc_stride\n",
    "# max_query_length = args.max_query_length\n",
    "# n_best_size = args.n_best_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inspect the SQuAD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Then we take a look at the Stanford Question Answering Dataset (SQuAD). The dataset can be downloaded using the `nlp.data.SQuAD` API. In this tutorial, we create a small dataset with 3 samples from the SQuAD dataset for demonstration purpose.\n",
    "\n",
    "The question answering task on the SQuAD dataset is setup the following way. For each sample in the dataset, a context is provided. The context is usually a long paragraph which contains lots of information. Then a question asked based on the context. The goal is to find the text span in the context that answers the question in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.555133Z",
     "start_time": "2019-06-14T01:45:27.418706Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/ec2-user/.mxnet/datasets/squad/dev-v1.1.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/squad/dev-v1.1.zip...\n",
      "Number of samples in the created dataset subsampled from SQuAD = 3\n"
     ]
    }
   ],
   "source": [
    "full_data = nlp.data.SQuAD(segment='dev', version='1.1')\n",
    "# loading a subset of the dev set of SQuAD\n",
    "num_target_samples = 3\n",
    "target_samples = [full_data[i] for i in range(num_target_samples)]\n",
    "dataset = mx.gluon.data.SimpleDataset(target_samples)\n",
    "print('Number of samples in the created dataset subsampled from SQuAD = %d'%len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's take a look at a sample from the dataset. In this sample, the question is about the location of the game, with a description about the Super Bowl 50 game as the context. Note that three different answer spans are correct for this question, and they start from index 403, 355 and 355 in the context respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.560564Z",
     "start_time": "2019-06-14T01:45:27.557274Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context:\n",
      "\n",
      "Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[2]\n",
    "\n",
    "context_idx = 3\n",
    "\n",
    "print('\\nContext:\\n')\n",
    "print(sample[context_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.567303Z",
     "start_time": "2019-06-14T01:45:27.562425Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question\n",
      "Where did Super Bowl 50 take place?\n",
      "\n",
      "Correct Answer Spans\n",
      "['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"]\n",
      "\n",
      "Answer Span Start Indices:\n",
      "[403, 355, 355]\n"
     ]
    }
   ],
   "source": [
    "question_idx = 2\n",
    "answer_idx = 4\n",
    "answer_pos_idx = 5\n",
    "\n",
    "print(\"\\nQuestion\")\n",
    "print(sample[question_idx])\n",
    "print(\"\\nCorrect Answer Spans\")\n",
    "print(sample[answer_idx])\n",
    "print(\"\\nAnswer Span Start Indices:\")\n",
    "print(sample[answer_pos_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Data Pre-processing for QA with BERT\n",
    "\n",
    "Recall that during BERT pre-training, it takes a sentence pair as the input, separated by the 'SEP' special token. For SQuAD, we can feed the context-question pair as the sentence pair input. To use BERT to predict the starting and ending span of the answer, we can add a classification layer for each token in the context texts, to predict if a token is the start or the end of the answer span. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:30:12.299493Z",
     "start_time": "2019-06-14T01:30:12.183419Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![qa](natural_language_understanding/qa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the next few code blocks, we will work on pre-processing the samples in the SQuAD dataset in the desired format with these special separators. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pre-trained BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, let's use the *get_model* API in GluonNLP to get the model definition for BERT, and the vocabulary used for the BERT model. Note that we discard the pooler and classifier layers used for the next sentence prediction task, as well as the decoder layers for the masked language model task during the BERT pre-training phase. These layers are not useful for predicting the starting and ending indices of the answer span.\n",
    "\n",
    "The list of pre-trained BERT models available in GluonNLP can be found [here](http://gluon-nlp.mxnet.io/model_zoo/bert/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.715444Z",
     "start_time": "2019-06-14T01:45:27.569118Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab file is not found. Downloading.\n",
      "Downloading /home/ec2-user/.mxnet/models/1578609291.58book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n"
     ]
    }
   ],
   "source": [
    "bert_model, vocab = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        use_classifier=False,\n",
    "                                        use_decoder=False,\n",
    "                                        use_pooler=False,\n",
    "                                        pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that there are several special tokens in the vocabulary for BERT. In particular, the `[SEP]` token is used for separating the sentence pairs, and the `[CLS]` token is added at the beginning of the sentence pairs. They will be used to pre-process the SQuAD dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.720137Z",
     "start_time": "2019-06-14T01:45:27.717192Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The second step is to process the samples using the same tokenizer used for BERT, which is provided as the `BERTTokenizer` API in GluonNLP. Note that instead of word level and character level representation, BERT uses subwords to represent a word, separated `##`. \n",
    "\n",
    "In the following example, the word `suspending` is tokenized as two subwords (`suspend` and `##ing`), and `numerals` is tokenized as three subwords (`nu`, `##meral`, `##s`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.731724Z",
     "start_time": "2019-06-14T01:45:27.721690Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'temporarily',\n",
       " 'suspend',\n",
       " '##ing',\n",
       " 'the',\n",
       " 'tradition',\n",
       " 'of',\n",
       " 'naming',\n",
       " 'each',\n",
       " 'super',\n",
       " 'bowl',\n",
       " 'game',\n",
       " 'with',\n",
       " 'roman',\n",
       " 'nu',\n",
       " '##meral',\n",
       " '##s']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nlp.data.BERTTokenizer(vocab=vocab, lower=True)\n",
    "\n",
    "tokenizer(\"as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentence Pair Composition\n",
    "\n",
    "With the tokenizer inplace, we are ready to process the question-context texts and compose sentence pairs. The functionality is available via the `SQuADTransform` API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.897684Z",
     "start_time": "2019-06-14T01:45:27.734029Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Transform dataset costs 0.19 seconds.\n"
     ]
    }
   ],
   "source": [
    "transform = bert.data.qa.SQuADTransform(tokenizer, is_pad=False, is_training=False, do_lookup=False)\n",
    "dev_data_transform, _ = bert.data.qa.preprocess_dataset(dataset, transform)\n",
    "logging.info('The number of examples after preprocessing:{}'.format(len(dev_data_transform)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take a look at the sample after the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.904353Z",
     "start_time": "2019-06-14T01:45:27.899992Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "segment type: \n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "text length: 168\n",
      "\n",
      "sentence pair: \n",
      "['[CLS]', 'where', 'did', 'super', 'bowl', '50', 'take', 'place', '?', '[SEP]', 'super', 'bowl', '50', 'was', 'an', 'american', 'football', 'game', 'to', 'determine', 'the', 'champion', 'of', 'the', 'national', 'football', 'league', '(', 'nfl', ')', 'for', 'the', '2015', 'season', '.', 'the', 'american', 'football', 'conference', '(', 'afc', ')', 'champion', 'denver', 'broncos', 'defeated', 'the', 'national', 'football', 'conference', '(', 'nfc', ')', 'champion', 'carolina', 'panthers', '24', 'â€“', '10', 'to', 'earn', 'their', 'third', 'super', 'bowl', 'title', '.', 'the', 'game', 'was', 'played', 'on', 'february', '7', ',', '2016', ',', 'at', 'levi', \"'\", 's', 'stadium', 'in', 'the', 'san', 'francisco', 'bay', 'area', 'at', 'santa', 'clara', ',', 'california', '.', 'as', 'this', 'was', 'the', '50th', 'super', 'bowl', ',', 'the', 'league', 'emphasized', 'the', '\"', 'golden', 'anniversary', '\"', 'with', 'various', 'gold', '-', 'themed', 'initiatives', ',', 'as', 'well', 'as', 'temporarily', 'suspend', '##ing', 'the', 'tradition', 'of', 'naming', 'each', 'super', 'bowl', 'game', 'with', 'roman', 'nu', '##meral', '##s', '(', 'under', 'which', 'the', 'game', 'would', 'have', 'been', 'known', 'as', '\"', 'super', 'bowl', 'l', '\"', ')', ',', 'so', 'that', 'the', 'logo', 'could', 'prominently', 'feature', 'the', 'arabic', 'nu', '##meral', '##s', '50', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sample = dev_data_transform[2]\n",
    "print('\\nsegment type: \\n' + str(sample[2]))\n",
    "print('\\ntext length: ' + str(sample[3]))\n",
    "print('\\nsentence pair: \\n' + str(sample[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocabulary Lookup\n",
    "\n",
    "Finally, we convert the transformed texts to subword indices, which are used to contructor NDArrays as the inputs to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.910853Z",
     "start_time": "2019-06-14T01:45:27.906127Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2073, 2106, 3565, 4605, 2753, 2202, 2173, 1029, 3, 3565, 4605, 2753, 2001, 2019, 2137, 2374, 2208, 2000, 5646, 1996, 3410, 1997, 1996, 2120, 2374, 2223, 1006, 5088, 1007, 2005, 1996, 2325, 2161, 1012, 1996, 2137, 2374, 3034, 1006, 10511, 1007, 3410, 7573, 14169, 3249, 1996, 2120, 2374, 3034, 1006, 22309, 1007, 3410, 3792, 12915, 2484, 1516, 2184, 2000, 7796, 2037, 2353, 3565, 4605, 2516, 1012, 1996, 2208, 2001, 2209, 2006, 2337, 1021, 1010, 2355, 1010, 2012, 11902, 1005, 1055, 3346, 1999, 1996, 2624, 3799, 3016, 2181, 2012, 4203, 10254, 1010, 2662, 1012, 2004, 2023, 2001, 1996, 12951, 3565, 4605, 1010, 1996, 2223, 13155, 1996, 1000, 3585, 5315, 1000, 2007, 2536, 2751, 1011, 11773, 11107, 1010, 2004, 2092, 2004, 8184, 28324, 2075, 1996, 4535, 1997, 10324, 2169, 3565, 4605, 2208, 2007, 3142, 16371, 28990, 2015, 1006, 2104, 2029, 1996, 2208, 2052, 2031, 2042, 2124, 2004, 1000, 3565, 4605, 1048, 1000, 1007, 1010, 2061, 2008, 1996, 8154, 2071, 14500, 3444, 1996, 5640, 16371, 28990, 2015, 2753, 1012, 3]\n"
     ]
    }
   ],
   "source": [
    "def vocab_lookup(example_id, subwords, type_ids, length, start, end):\n",
    "    indices = vocab[subwords]\n",
    "    return example_id, indices, type_ids, length, start, end\n",
    "\n",
    "dev_data_transform = dev_data_transform.transform(vocab_lookup, lazy=False)\n",
    "print(dev_data_transform[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "After the data is processed, we can define the model that uses the representation produced by BERT for predicting the starting and ending positions of the answer span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:27.921076Z",
     "start_time": "2019-06-14T01:45:27.912650Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We download a BERT model trained on the SQuAD dataset, prepare the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.221383Z",
     "start_time": "2019-06-14T01:45:27.922825Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ./temp/bert_qa-7eb11865.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_qa-7eb11865.zip...\n",
      "Downloaded checkpoint to ./temp/bert_qa-7eb11865.params\n"
     ]
    }
   ],
   "source": [
    "net = BertForQA(bert_model)\n",
    "ctx = mx.gpu(0)\n",
    "ckpt = qa_utils.download_qa_ckpt()\n",
    "net.load_parameters(ckpt, ctx=ctx)\n",
    "\n",
    "batch_size = 1\n",
    "dev_dataloader = mx.gluon.data.DataLoader(\n",
    "    dev_data_transform, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.575976Z",
     "start_time": "2019-06-14T01:45:32.223336Z"
    }
   },
   "outputs": [],
   "source": [
    "all_results = collections.defaultdict(list)\n",
    "\n",
    "total_num = 0\n",
    "for data in dev_dataloader:\n",
    "    example_ids, inputs, token_types, valid_length, _, _ = data\n",
    "    total_num += len(inputs)\n",
    "    batch_size = inputs.shape[0]\n",
    "    output = net(inputs.astype('float32').as_in_context(ctx),\n",
    "                               token_types.astype('float32').as_in_context(ctx),\n",
    "                               valid_length.astype('float32').as_in_context(ctx))\n",
    "    pred_start, pred_end = mx.nd.split(output, axis=2, num_outputs=2)\n",
    "    example_ids = example_ids.asnumpy().tolist()\n",
    "    pred_start = pred_start.reshape(batch_size, -1).asnumpy()\n",
    "    pred_end = pred_end.reshape(batch_size, -1).asnumpy()\n",
    "    \n",
    "    for example_id, start, end in zip(example_ids, pred_start, pred_end):\n",
    "        all_results[example_id].append(PredResult(start=start, end=end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T01:45:32.623482Z",
     "start_time": "2019-06-14T01:45:32.578002Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: which nfl team represented the afc at super bowl 50 ?\n",
      "\n",
      "Top predictions: \n",
      "99.36% \t Denver Broncos\n",
      "0.23% \t The American Football Conference (AFC) champion Denver Broncos\n",
      "0.20% \t Broncos\n",
      "\n",
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: which nfl team represented the nfc at super bowl 50 ?\n",
      "\n",
      "Top predictions: \n",
      "66.65% \t Carolina Panthers\n",
      "24.30% \t Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers\n",
      "7.42% \t Denver Broncos\n",
      "\n",
      "\n",
      "Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "\n",
      "Question: where did super bowl 50 take place ?\n",
      "\n",
      "Top predictions: \n",
      "25.86% \t Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\n",
      "23.11% \t Levi's Stadium\n",
      "17.88% \t San Francisco Bay Area at Santa Clara, California\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_utils.predict(dataset, all_results, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Train the Model\n",
    "\n",
    "Now we can put all the pieces together, and start fine-tuning the model with a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = BertForQA(bert=bert_model)\n",
    "# nlp.utils.load_parameters(net, pretrained_bert_parameters, ctx=ctx,\n",
    "#                           ignore_extra=True, cast_dtype=True)\n",
    "net.span_classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "net.hybridize(static_alloc=True)\n",
    "\n",
    "loss_function = BertForQALoss()\n",
    "loss_function.hybridize(static_alloc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Stack(),\n",
    "    nlp.data.batchify.Pad(axis=0, pad_val=vocab[vocab.padding_token]),\n",
    "    nlp.data.batchify.Pad(axis=0, pad_val=vocab[vocab.padding_token]),\n",
    "    nlp.data.batchify.Stack('float32'),\n",
    "    nlp.data.batchify.Stack('float32'),\n",
    "    nlp.data.batchify.Stack('float32'))\n",
    "\n",
    "np.random.seed(6)\n",
    "random.seed(6)\n",
    "mx.random.seed(6)\n",
    "\n",
    "log = logging.getLogger('gluonnlp')\n",
    "log.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\n",
    "    fmt='%(levelname)s:%(name)s:%(asctime)s %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "\n",
    "segment = 'train'  # if not args.debug else 'dev'\n",
    "log.info('Loading %s data...', segment)\n",
    "#     if version_2:\n",
    "#         train_data = SQuAD(segment, version='2.0')\n",
    "#     else:\n",
    "train_data = SQuAD(segment, version='1.1')\n",
    "#     if args.debug:\n",
    "#         sampled_data = [train_data[i] for i in range(1000)]\n",
    "#         train_data = mx.gluon.data.SimpleDataset(sampled_data)\n",
    "log.info('Number of records in Train data:{}'.format(len(train_data)))\n",
    "\n",
    "train_data_transform, _ = preprocess_dataset(\n",
    "    train_data, SQuADTransform(\n",
    "        copy.copy(tokenizer),\n",
    "        max_seq_length=max_seq_length,\n",
    "        doc_stride=doc_stride,\n",
    "        max_query_length=max_query_length,\n",
    "        is_pad=True,\n",
    "        is_training=True))\n",
    "log.info('The number of examples after preprocessing:{}'.format(\n",
    "    len(train_data_transform)))\n",
    "\n",
    "train_dataloader = mx.gluon.data.DataLoader(\n",
    "    train_data_transform, batchify_fn=batchify_fn,\n",
    "    batch_size=batch_size, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(log, train_data_transform, train_dataloader):\n",
    "    \"\"\"Training function.\"\"\"\n",
    "\n",
    "    log.info('Start Training')\n",
    "\n",
    "    optimizer_params = {'learning_rate': lr}\n",
    "    trainer = mx.gluon.Trainer(net.collect_params(), optimizer,\n",
    "                               optimizer_params, update_on_kvstore=False)\n",
    "\n",
    "    num_train_examples = len(train_data_transform)\n",
    "    step_size = batch_size * accumulate if accumulate else batch_size\n",
    "    num_train_steps = int(num_train_examples / step_size * epochs)\n",
    "    num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
    "    step_num = 0\n",
    "    \n",
    "    def set_new_lr(step_num, batch_id):\n",
    "        \"\"\"set new learning rate\"\"\"\n",
    "        # set grad to zero for gradient accumulation\n",
    "        if accumulate:\n",
    "            if batch_id % accumulate == 0:\n",
    "                net.collect_params().zero_grad()\n",
    "                step_num += 1\n",
    "        else:\n",
    "            step_num += 1\n",
    "        # learning rate schedule\n",
    "        # Notice that this learning rate scheduler is adapted from traditional linear learning\n",
    "        # rate scheduler where step_num >= num_warmup_steps, new_lr = 1 - step_num/num_train_steps\n",
    "        if step_num < num_warmup_steps:\n",
    "            new_lr = lr * step_num / num_warmup_steps\n",
    "        else:\n",
    "            offset = (step_num - num_warmup_steps) * lr / \\\n",
    "                (num_train_steps - num_warmup_steps)\n",
    "            new_lr = lr - offset\n",
    "        trainer.set_learning_rate(new_lr)\n",
    "        return step_num\n",
    "\n",
    "    # Do not apply weight decay on LayerNorm and bias terms\n",
    "    for _, v in net.collect_params('.*beta|.*gamma|.*bias').items():\n",
    "        v.wd_mult = 0.0\n",
    "    # Collect differentiable parameters\n",
    "    params = [p for p in net.collect_params().values()\n",
    "              if p.grad_req != 'null']\n",
    "    # Set grad_req if gradient accumulation is required\n",
    "    if accumulate:\n",
    "        for p in params:\n",
    "            p.grad_req = 'add'\n",
    "\n",
    "    epoch_tic = time.time()\n",
    "    total_num = 0\n",
    "    log_num = 0\n",
    "    for epoch_id in range(epochs):\n",
    "        step_loss = 0.0\n",
    "        tic = time.time()\n",
    "        for batch_id, data in enumerate(train_dataloader):\n",
    "            # set new lr\n",
    "            step_num = set_new_lr(step_num, batch_id)\n",
    "            # forward and backward\n",
    "            with mx.autograd.record():\n",
    "                _, inputs, token_types, valid_length, start_label, end_label = data\n",
    "\n",
    "                log_num += len(inputs)\n",
    "                total_num += len(inputs)\n",
    "\n",
    "                out = net(inputs.astype('float32').as_in_context(ctx),\n",
    "                          token_types.astype('float32').as_in_context(ctx),\n",
    "                          valid_length.astype('float32').as_in_context(ctx))\n",
    "\n",
    "                ls = loss_function(out, [\n",
    "                    start_label.astype('float32').as_in_context(ctx),\n",
    "                    end_label.astype('float32').as_in_context(ctx)]).mean()\n",
    "\n",
    "                if accumulate:\n",
    "                    ls = ls / accumulate\n",
    "            ls.backward()\n",
    "            # update\n",
    "            if not accumulate or (batch_id + 1) % accumulate == 0:\n",
    "                trainer.allreduce_grads()\n",
    "                nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                trainer.update(1)\n",
    "\n",
    "            step_loss += ls.asscalar()\n",
    "        \n",
    "#         for batch_id, data in enumerate(train_dataloader):\n",
    "#             # set new lr\n",
    "#             step_num = set_new_lr(step_num, batch_id)\n",
    "#             # forward and backward\n",
    "# #             with mx.autograd.record():\n",
    "#             _, inputs, token_types, valid_length, start_label, end_label = data\n",
    "\n",
    "#             log_num += len(inputs)\n",
    "#             total_num += len(inputs)\n",
    "\n",
    "#             def split_and_load(data, ctx):\n",
    "#                 n, k = data.shape[0], len(ctx)\n",
    "#                 print(n, k)\n",
    "#                 if (n//k)*k != n:\n",
    "#                     drop = n - (n//k)*k\n",
    "#                     data = data[:-drop]\n",
    "#                     n, k = data.shape[0], len(ctx)\n",
    "#                 assert (n//k)*k == n, '# examples is not divided by # devices'\n",
    "#                 idx = list(range(0, n+1, n//k))\n",
    "#                 return [data[idx[i]:idx[i+1]].as_in_context(ctx[i]) for i in range(k)]\n",
    "            \n",
    "# #                 def train_batch(inputs, params, ctx, lr):\n",
    "#                     # split the data batch and load them on GPUs\n",
    "#             print(len(inputs[0]), len(token_types[0]), len(valid_length[0]), len(start_label[0]), len(end_label[0]))\n",
    "#             inputs = split_and_load(inputs[0], ctx)\n",
    "#             token_types = split_and_load(token_types[0], ctx)\n",
    "# #             valid_length = split_and_load(valid_length, ctx)\n",
    "# #             start_label = split_and_load(start_label[0], ctx)\n",
    "# #             end_label = split_and_load(end_label[0], ctx)\n",
    "\n",
    "#             # run forward on each GPU\n",
    "#             with mx.autograd.record():\n",
    "#                 losses = [loss_function(net(X, Y, W), [U, V])\n",
    "#                           for X, Y, W, U, V in zip(inputs, token_types, valid_length, start_label, end_label)]\n",
    "#             # run backward on each gpu\n",
    "#             for ls in losses:\n",
    "#                 ls.backward()\n",
    "#                 step_loss += ls.asscalar()\n",
    "#             # aggregate gradient over GPUs\n",
    "#             for i in range(len(params[0])):\n",
    "#                 allreduce([params[c][i].grad for c in range(len(ctx))])\n",
    "#             # update parameters with SGD on each GPU\n",
    "#             for p in params:\n",
    "#                 nlp.utils.clip_grad_global_norm(P, 1)\n",
    "#                 trainer.update(1)\n",
    "            if (batch_id + 1) % log_interval == 0: \n",
    "                toc = time.time()\n",
    "                log.info('Epoch: {}, Batch: {}/{}, Loss={:.4f}, lr={:.7f} Time cost={:.1f} Thoughput={:.2f} samples/s'  # pylint: disable=line-too-long\n",
    "                         .format(epoch_id, batch_id, len(train_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, toc - tic, log_num/(toc - tic)))\n",
    "                tic = time.time()\n",
    "                step_loss = 0.0\n",
    "                log_num = 0\n",
    "        epoch_toc = time.time()\n",
    "        log.info('Time cost={:.2f} s, Thoughput={:.2f} samples/s'.format(\n",
    "            epoch_toc - epoch_tic, total_num/(epoch_toc - epoch_tic)))\n",
    "\n",
    "    net.save_parameters(os.path.join(output_dir, 'net.params'))\n",
    "\n",
    "\n",
    "train(log, train_data_transform, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \"\"\"Evaluate the model on validation dataset.\n",
    "    \"\"\"\n",
    "    log.info('Loading dev data...')\n",
    "#     if version_2:\n",
    "#         dev_data = SQuAD('dev', version='2.0')\n",
    "#     else:\n",
    "    dev_data = SQuAD('dev', version='1.1')\n",
    "    if args.debug:\n",
    "        sampled_data = [dev_data[0], dev_data[1], dev_data[2]]\n",
    "        dev_data = mx.gluon.data.SimpleDataset(sampled_data)\n",
    "    log.info('Number of records in dev data:{}'.format(len(dev_data)))\n",
    "\n",
    "    dev_dataset = dev_data.transform(\n",
    "        SQuADTransform(\n",
    "            copy.copy(tokenizer),\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_pad=False,\n",
    "            is_training=False)._transform, lazy=False)\n",
    "    \n",
    "    dev_data_transform, _ = preprocess_dataset(\n",
    "        dev_data, SQuADTransform(\n",
    "            copy.copy(tokenizer),\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_pad=False,\n",
    "            is_training=False))\n",
    "    log.info('The number of examples after preprocessing:{}'.format(\n",
    "        len(dev_data_transform)))\n",
    "\n",
    "    dev_dataloader = mx.gluon.data.DataLoader(\n",
    "        dev_data_transform,\n",
    "        batchify_fn=batchify_fn,\n",
    "        num_workers=4, batch_size=test_batch_size,\n",
    "        shuffle=False, last_batch='keep')\n",
    "\n",
    "    log.info('start prediction')\n",
    "\n",
    "    all_results = collections.defaultdict(list)\n",
    "\n",
    "    epoch_tic = time.time()\n",
    "    total_num = 0\n",
    "    for data in dev_dataloader:\n",
    "        example_ids, inputs, token_types, valid_length, _, _ = data\n",
    "        total_num += len(inputs)\n",
    "        out = net(inputs.astype('float32').as_in_context(ctx),\n",
    "                  token_types.astype('float32').as_in_context(ctx),\n",
    "                  valid_length.astype('float32').as_in_context(ctx))\n",
    "\n",
    "        output = mx.nd.split(out, axis=2, num_outputs=2)\n",
    "        example_ids = example_ids.asnumpy().tolist()\n",
    "        pred_start = output[0].reshape((0, -3)).asnumpy()\n",
    "        pred_end = output[1].reshape((0, -3)).asnumpy()\n",
    "\n",
    "        for example_id, start, end in zip(example_ids, pred_start, pred_end):\n",
    "            all_results[example_id].append(PredResult(start=start, end=end))\n",
    "            \n",
    "    epoch_toc = time.time()\n",
    "    log.info('Time cost={:.2f} s, Thoughput={:.2f} samples/s'.format(\n",
    "        epoch_toc - epoch_tic, total_num/(epoch_toc - epoch_tic)))\n",
    "\n",
    "    log.info('Get prediction results...')\n",
    "\n",
    "    all_predictions = collections.OrderedDict()\n",
    "\n",
    "    for features in dev_dataset:\n",
    "        results = all_results[features[0].example_id]\n",
    "        example_qas_id = features[0].qas_id\n",
    "\n",
    "        prediction, _ = predict(\n",
    "            features=features,\n",
    "            results=results,\n",
    "            tokenizer=nlp.data.BERTBasicTokenizer(lower=lower),\n",
    "            max_answer_length=max_answer_length,\n",
    "            null_score_diff_threshold=null_score_diff_threshold,\n",
    "            n_best_size=n_best_size,\n",
    "#             version_2=version_2\n",
    "        )\n",
    "\n",
    "        all_predictions[example_qas_id] = prediction\n",
    "\n",
    "    with io.open(os.path.join(output_dir, 'predictions.json'),\n",
    "                 'w', encoding='utf-8') as fout:\n",
    "        data = json.dumps(all_predictions, ensure_ascii=False)\n",
    "        fout.write(data)\n",
    "\n",
    "#     if version_2:\n",
    "#         log.info('Please run evaluate-v2.0.py to get evaluation results for SQuAD 2.0')\n",
    "#     else:\n",
    "    F1_EM = get_F1_EM(dev_data, all_predictions)\n",
    "    log.info(F1_EM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy on SageMaker\n",
    "\n",
    "1. Saving the model parameters\n",
    "2. Preparing functions for inference \n",
    "3. Building a docker container with dependencies installed\n",
    "4. Launch a serving end-point with SageMaker SDK\n",
    "\n",
    "### 1. Save the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save parameters, model definition and vocabulary in a zip file\n",
    "\n",
    "# net.export('checkpoint')\n",
    "\n",
    "output_dir = \"model_outputs\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "with open(output_dir+'/vocab.json', 'w') as f:\n",
    "    f.write(vocab.to_json())\n",
    "import tarfile\n",
    "with tarfile.open(output_dir+\"/model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"./temp/bert_qa-7eb11865.params\")\n",
    "#     tar.add(\"output_dir/checkpoint-0000.params\") \n",
    "#     tar.add(\"output_dir/checkpoint-symbol.json\") ???\n",
    "    tar.add(output_dir+\"/vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparing functions for inference\n",
    "\n",
    "Two functions: \n",
    "1. model_fn() to load model parameters\n",
    "2. transform_fn() to run model inference given an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building a docker container with dependencies installed\n",
    "\n",
    "Let's prepare a docker container with all the dependencies required for model inference. Here we build a docker container based on the SageMaker MXNet inference container, and you can find the list of all available inference containers at https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html\n",
    "\n",
    "Here we use local mode for demonstration purpose. To deploy on actual instances, you need to login into AWS elastic container registry (ECR) service, and push the container to ECR. \n",
    "\n",
    "```\n",
    "docker build -t $YOUR_EDR_DOCKER_TAG . -f Dockerfile\n",
    "$(aws ecr get-login --no-include-email --region $YOUR_REGION)\n",
    "docker push $YOUR_EDR_DOCKER_TAG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "ARG REGION\n",
    "FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/mxnet-inference:1.6.0-gpu-py3\n",
    "\n",
    "RUN pip install --upgrade --user --pre 'mxnet-mkl' 'https://github.com/dmlc/gluon-nlp/tarball/v0.9.x'\n",
    "\n",
    "RUN pip list | grep mxnet\n",
    "\n",
    "COPY *.py /opt/ml/model/code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker login cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!$(aws ecr get-login --no-include-email --region us-east-1 --registry-ids 763104351884)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  843.8MB\n",
      "Step 1/5 : ARG REGION\n",
      "Step 2/5 : FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/mxnet-inference:1.6.0-gpu-py3\n",
      "1.6.0-gpu-py3: Pulling from mxnet-inference\n",
      "\n",
      "\u001b[1B7927d38a: Pulling fs layer \n",
      "\u001b[1Bac894db4: Pulling fs layer \n",
      "\u001b[1B2af6d627: Pulling fs layer \n",
      "\u001b[1B86211d23: Pulling fs layer \n",
      "\u001b[1B603ff777: Pulling fs layer \n",
      "\u001b[1B7165632f: Pulling fs layer \n",
      "\u001b[1B96e40dcf: Pulling fs layer \n",
      "\u001b[1B91ff3706: Pulling fs layer \n",
      "\u001b[1B02a4385b: Pulling fs layer \n",
      "\u001b[1Be229cfdb: Pulling fs layer \n",
      "\u001b[1B0e6ed5b5: Pulling fs layer \n",
      "\u001b[1Bc8e328fe: Pulling fs layer \n",
      "\u001b[1Bbb20abb1: Pulling fs layer \n",
      "\u001b[1B0702cb67: Pulling fs layer \n",
      "\u001b[1Bd6c2671b: Pulling fs layer \n",
      "\u001b[1B486e676d: Pulling fs layer \n",
      "\u001b[10B1ff3706: Waiting fs layer \n",
      "\u001b[1B7d871d5a: Pulling fs layer \n",
      "\u001b[8Bc8e328fe: Waiting fs layer \n",
      "\u001b[8Bbb20abb1: Waiting fs layer \n",
      "\u001b[1Bc02fa024: Pulling fs layer \n",
      "\u001b[15B1ff3706: Extracting  688.7MB/688.7MBBK\u001b[19A\u001b[1K\u001b[K\u001b[18A\u001b[1K\u001b[K\u001b[18A\u001b[1K\u001b[K\u001b[16A\u001b[1K\u001b[K\u001b[17A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[22A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[22A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[22A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[22A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[22A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[22A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[22A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[22A\u001b[1K\u001b[K\u001b[22A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[22A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[22A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[22A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[20A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[19A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[18A\u001b[1K\u001b[K\u001b[18A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[18A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[17A\u001b[1K\u001b[K\u001b[17A\u001b[1K\u001b[K\u001b[17A\u001b[1K\u001b[K\u001b[17A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[2A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[Kfailed to register layer: Error processing tar file(exit status 1): write /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcusolver.so.10.2.0.243: no space left on device\n"
     ]
    }
   ],
   "source": [
    "!export REGION=$(wget -qO- http://169.254.169.254/latest/meta-data/placement/availability-zone) &&\\\n",
    " docker build --no-cache --build-arg REGION=${REGION::-1} -t my-docker:inference . -f Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qO- http://169.254.169.254/latest/meta-data/placement/availability-zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws ecr get-login --no-include-email --registry-ids 763104351884\n",
    "!aws ecr get-login --no-include-email --region us-east-1 --registry-ids 763104351884"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/mxnet-inference:1.6.0-gpu-py3\n",
    "#              763104351884.dkr.ecr.<region>.amazonaws.com/mxnet-inference:1.4.1-gpu-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ecr get-login --no-include-email --region us-east-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull 520713654638.dkr.ecr.us-east-1.amazonaws.com/mxnet-inference:1.6.0-gpu-py3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "role=sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ecr get-login --no-include-email --registry-ids 520713654638"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
