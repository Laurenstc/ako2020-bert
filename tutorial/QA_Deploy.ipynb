{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on: Deploying Question Answering with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained language representations have been shown to improve many downstream NLP tasks such as question answering, and natural language inference. Devlin, Jacob, et al proposed BERT [1] (Bidirectional Encoder Representations from Transformers), which fine-tunes deep bidirectional representations on a wide range of tasks with minimal task-specific parameters, and obtained state- of-the-art results.\n",
    "\n",
    "After finishing training QA with BERT (the previous notebook \"QA_Training.ipydb\"), let us load a trained model to perform inference on the SQuAD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick overview: an example from SQuAD dataset is like below:\n",
    "\n",
    "    (2, \n",
    "    '56be4db0acb8001400a502ee', \n",
    "    'Where did Super Bowl 50 take place?', \n",
    "\n",
    "    'Super Bowl 50 was an American football game to determine the champion of the National \n",
    "    Football League (NFL) for the 2015 season. The American Football Conference (AFC) \n",
    "    champion Denver Broncos defeated the National Football Conference (NFC) champion \n",
    "    Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played \n",
    "    on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, \n",
    "    California. As this was the 50th Super Bowl, the league emphasized the \"golden \n",
    "    anniversary\" with various gold-themed initiatives, as well as temporarily suspending \n",
    "    the tradition of naming each Super Bowl game with Roman numerals (under which the \n",
    "    game would have been known as \"Super Bowl L\"), so that the logo could prominently \n",
    "    feature the Arabic numerals 50.', \n",
    "\n",
    "    ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium \n",
    "    in the San Francisco Bay Area at Santa Clara, California.\"], \n",
    "\n",
    "    [403, 355, 355])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy on SageMaker\n",
    "\n",
    "1. Preparing functions for inference \n",
    "2. Saving the model parameters\n",
    "3. Building a docker container with dependencies installed\n",
    "4. Launching a serving end-point with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing functions for inference\n",
    "\n",
    "Two functions: \n",
    "1. model_fn() to load model parameters\n",
    "2. transform_fn() to run model inference given an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile code/serve.py\n",
    "import collections, json, logging, warnings\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import Block, nn\n",
    "from bert.data.qa import preprocess_dataset, SQuADTransform\n",
    "import bert_qa_evaluate\n",
    "\n",
    "\n",
    "class BertForQA(Block):\n",
    "    \"\"\"Model for SQuAD task with BERT.\n",
    "    The model feeds token ids and token type ids into BERT to get the\n",
    "    pooled BERT sequence representation, then apply a Dense layer for QA task.\n",
    "    Parameters\n",
    "    ----------\n",
    "    bert: BERTModel\n",
    "        Bidirectional encoder with transformer.\n",
    "    prefix : str or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    params : ParameterDict or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert, prefix=None, params=None):\n",
    "        super(BertForQA, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.span_classifier = nn.Dense(units=2, flatten=False)\n",
    "\n",
    "    def forward(self, inputs, token_types, valid_length=None):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Generate the unnormalized score for the given the input sequences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : NDArray, shape (batch_size, seq_length)\n",
    "            Input words for the sequences.\n",
    "        token_types : NDArray, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one.\n",
    "        valid_length : NDArray or None, shape (batch_size,)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : NDArray\n",
    "            Shape (batch_size, seq_length, 2)\n",
    "        \"\"\"\n",
    "        bert_output = self.bert(inputs, token_types, valid_length)\n",
    "        output = self.span_classifier(bert_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def model_fn(model_dir = \"\", params_path = \"bert_qa-7eb11865.params\"):\n",
    "    \"\"\"\n",
    "    Load the gluon model. Called once when hosting service starts.\n",
    "    :param: model_dir The directory where model files are stored.\n",
    "    :return: a Gluon model, and the vocabulary\n",
    "    \"\"\"\n",
    "    bert_model, vocab = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        use_classifier=False,\n",
    "                                        use_decoder=False,\n",
    "                                        use_pooler=False,\n",
    "                                        pretrained=False)\n",
    "    net = BertForQA(bert_model)\n",
    "    if len(model_dir) > 0:\n",
    "        params_path = model_dir + \"/\" +params_path\n",
    "    net.load_parameters(params_path, ctx=mx.cpu())\n",
    "    \n",
    "    tokenizer = nlp.data.BERTTokenizer(vocab,  lower=True)\n",
    "    transform = SQuADTransform(tokenizer, is_pad=False, is_training=False, do_lookup=False)\n",
    "    return net, vocab, transform\n",
    "\n",
    "\n",
    "def transform_fn(model, input_data, input_content_type=None, output_content_type=None):\n",
    "    \"\"\"\n",
    "    Transform a request using the Gluon model. Called once per request.\n",
    "    :param model: The Gluon model and the vocab\n",
    "    :param dataset: The request payload\n",
    "    \n",
    "        Example:\n",
    "        ## (example_id, [question, content], ques_cont_token_types, valid_length, _, _)\n",
    "\n",
    "    :param input_content_type: The request content type, assume json\n",
    "    :param output_content_type: The (desired) response content type, assume json\n",
    "    :return: response payload and content type.\n",
    "    \"\"\"\n",
    "    net, vocab, squadTransform = model\n",
    "    data = json.loads(input_data)\n",
    "    test_examples_tuples = bert_qa_evaluate._test_example_transform(data)\n",
    "    test_dataset = mx.gluon.data.SimpleDataset(test_examples_tuples)\n",
    "    all_results = bert_qa_evaluate.get_all_results(net, vocab, squadTransform, test_dataset, ctx=mx.cpu())\n",
    "    all_predictions = collections.defaultdict(list)\n",
    "    data_transform = test_dataset.transform(squadTransform._transform)\n",
    "    for features in data_transform:\n",
    "        f_id = features[0].example_id\n",
    "        results = all_results[f_id]\n",
    "        prediction, nbest = bert_qa_evaluate.predict(\n",
    "            features=features,\n",
    "            results=results,\n",
    "            tokenizer=nlp.data.BERTBasicTokenizer(vocab))        \n",
    "        nbest_prediction = [] \n",
    "        for i in range(3):\n",
    "            nbest_prediction.append('%.2f%% \\t %s'%(nbest[i][1] * 100, nbest[i][0]))\n",
    "        all_predictions[f_id] = nbest_prediction\n",
    "    response_body = json.dumps(all_predictions)\n",
    "    return response_body, output_content_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Saving the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model parameters and vocabulary in a zip file\n",
    "\n",
    "import tarfile\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "#     tar.add(\"Dockerfile\")\n",
    "    tar.add(\"code/serve.py\")\n",
    "    tar.add(\"bert/data/qa.py\")\n",
    "    tar.add(\"bert_qa_evaluate.py\")\n",
    "    tar.add(\"bert_qa-7eb11865.params\")\n",
    "    tar.add(\"vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "my_test_example_0 = ('Which NFL team represented the AFC at Super Bowl 50?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_example_1 = ('Where did Super Bowl 50 take place?',\n",
    " 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.')\n",
    "\n",
    "my_test_examples = (my_test_example_0, my_test_example_1)\n",
    "\n",
    "# mymodel = model_fn(params_path = \"bert_qa-7eb11865.params\")\n",
    "# transform_fn(mymodel, my_test_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building a docker container with dependencies installed\n",
    "\n",
    "Let's prepare a docker container with all the dependencies required for model inference. Here we build a docker container based on the SageMaker MXNet inference container, and you can find the list of all available inference containers at https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html\n",
    "\n",
    "Here we use local mode for demonstration purpose. To deploy on actual instances, you need to login into AWS elastic container registry (ECR) service, and push the container to ECR. \n",
    "\n",
    "```\n",
    "docker build -t $YOUR_EDR_DOCKER_TAG . -f Dockerfile\n",
    "$(aws ecr get-login --no-include-email --region $YOUR_REGION)\n",
    "docker push $YOUR_EDR_DOCKER_TAG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "ARG REGION\n",
    "FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/mxnet-inference:1.4.1-gpu-py3\n",
    "\n",
    "RUN pip install --upgrade --user --pre 'mxnet-mkl' 'https://github.com/dmlc/gluon-nlp/tarball/v0.9.x'\n",
    "\n",
    "RUN pip list | grep mxnet\n",
    "\n",
    "COPY *.py /opt/ml/model/code/\n",
    "COPY bert/data/qa.py /opt/ml/model/code/bert/data/\n",
    "COPY bert/bert_qa_evaluate.py /opt/ml/model/code/bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  843.5MB\n",
      "Step 1/7 : ARG REGION\n",
      "Step 2/7 : FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/mxnet-inference:1.4.1-gpu-py3\n",
      " ---> d9dd4dcfe0c2\n",
      "Step 3/7 : RUN pip install --upgrade --user --pre 'mxnet-mkl' 'https://github.com/dmlc/gluon-nlp/tarball/v0.9.x'\n",
      " ---> Running in 74f0ee0ac65f\n",
      "Collecting https://github.com/dmlc/gluon-nlp/tarball/v0.9.x\n",
      "  Downloading https://github.com/dmlc/gluon-nlp/tarball/v0.9.x\n",
      "Collecting mxnet-mkl\n",
      "  Downloading https://files.pythonhosted.org/packages/64/72/c5566aabde6ee0bda1f09d026603169a717dbd9f26f6be85ee2b4ed2cf03/mxnet_mkl-1.6.0b20191025-py2.py3-none-manylinux1_x86_64.whl (64.9MB)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/site-packages (from gluonnlp==0.9.0.dev0) (1.14.6)\n",
      "Requirement already satisfied, skipping upgrade: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/site-packages (from mxnet-mkl) (0.8.4)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.6/site-packages (from mxnet-mkl) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-mkl) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-mkl) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-mkl) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-mkl) (1.25.3)\n",
      "\u001b[91mERROR: mxnet-mkl 1.6.0b20191025 has requirement numpy<2.0.0,>1.16.0, but you'll have numpy 1.14.6 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: mxnet-mkl, gluonnlp\n",
      "  Running setup.py install for gluonnlp: started\n",
      "    Running setup.py install for gluonnlp: finished with status 'done'\n",
      "Successfully installed gluonnlp-0.9.0.dev0 mxnet-mkl-1.6.0b20191025\n",
      "\u001b[91mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 74f0ee0ac65f\n",
      " ---> bc3bcf04918d\n",
      "Step 4/7 : RUN pip list | grep mxnet\n",
      " ---> Running in 85ce4ab37b38\n",
      "\u001b[91mWARNING: You are using pip version 19.1.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mkeras-mxnet                       2.2.4.1       \n",
      "mxnet-cu100mkl                    1.4.1         \n",
      "mxnet-mkl                         1.6.0b20191025\n",
      "mxnet-model-server                1.0.4         \n",
      "sagemaker-mxnet-serving-container 1.0.0         \n",
      "Removing intermediate container 85ce4ab37b38\n",
      " ---> 728e067b5362\n",
      "Step 5/7 : COPY *.py /opt/ml/model/code/\n",
      " ---> dc979c438032\n",
      "Step 6/7 : COPY bert/data/qa.py /opt/ml/model/code/bert/data/\n",
      " ---> 83a2ab72386b\n",
      "Step 7/7 : COPY bert/bert_qa_evaluate.py /opt/ml/model/code/bert/\n",
      " ---> 88cd6f02b1d3\n",
      "Successfully built 88cd6f02b1d3\n",
      "Successfully tagged my-docker:inference\n"
     ]
    }
   ],
   "source": [
    "!export REGION=$(wget -qO- http://169.254.169.254/latest/meta-data/placement/availability-zone) &&\\\n",
    " docker build --no-cache --build-arg REGION=${REGION::-1} -t my-docker:inference . -f Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Launching a serving end-point with SageMaker SDK\n",
    "\n",
    "We create a MXNet model which can be deployed later, by specifying the docker image, and entry point for the inference code. If serve.py does not work, use dummy_hosting_module.py for debugging purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "sagemaker_model = MXNetModel(model_data='file:///home/ec2-user/SageMaker/ako2020-bert/tutorial/model.tar.gz',\n",
    "                             image='my-docker:inference', # docker images\n",
    "                             role=sagemaker.get_execution_role(), \n",
    "                             py_version='py3',            # python version\n",
    "                             entry_point='serve.py',\n",
    "                             source_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 'local' mode to test our deployment code, where the inference happens on the current instance.\n",
    "If you are ready to deploy the model on a new instance, change the `instance_type` argument to values such as `ml.c4.xlarge`.\n",
    "\n",
    "Here we use 'local' mode for testing, for real instances use c5.2xlarge, p2.xlarge, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmp_f5n_fa7_algo-1-jf0uq_1\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,352 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m MMS Home: /usr/local/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Current directory: /\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Number of CPUs: 8\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Max heap size: 13646 M\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Python executable: /usr/local/bin/python3.6\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Config file: /etc/sagemaker-mms.properties\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Management address: http://127.0.0.1:8081\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Model Store: /.sagemaker/mms/models\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Initial Models: ALL\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Log dir: /logs\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Netty threads: 0\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Netty client threads: 0\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Default workers per model: 8\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,414 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,432 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,616 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9001\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,617 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]78\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,621 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9003\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,622 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]80\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,623 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,623 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,627 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,629 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,631 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9004\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,632 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]76\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,633 [INFO ] W-9003-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9003\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,633 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9001\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,640 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9005\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,640 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]82\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,641 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,641 [INFO ] W-9005-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9005\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,641 [INFO ] W-9004-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9004\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,641 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,642 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,643 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,649 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9006\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,649 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]83\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,650 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,650 [INFO ] W-9006-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9006\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,650 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,653 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9007\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,653 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]79\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,653 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,653 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9007\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,653 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,657 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,657 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Management server with: EpollServerSocketChannel.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,658 [INFO ] main com.amazonaws.ml.mms.ModelServer - Management API bind to: http://127.0.0.1:8081\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,659 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,659 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]81\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,660 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,660 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,660 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m Model server started.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,663 [WARN ] pool-2-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,664 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,664 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9005.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,664 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9007.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,670 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9003.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,672 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9001.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,672 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9006.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,672 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9004.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,677 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9002\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,677 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]77\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,677 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,677 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9002\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,677 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,680 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:54:59,926 [INFO ] pool-1-thread-9 ACCESS_LOG - /172.18.0.1:60880 \"GET /ping HTTP/1.1\" 200 36\r\n",
      "!"
     ]
    }
   ],
   "source": [
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:55:02,760 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3045\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:55:02,784 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3073\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:55:02,788 [INFO ] W-9005-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3095\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:55:02,796 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Vocab file is not found. Downloading.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:55:02,796 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Downloading /root/.mxnet/models/1579049702.7962046book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:55:02,796 [INFO ] W-9003-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3081\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:55:02,805 [INFO ] W-9006-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3106\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:55:02,817 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3113\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:55:02,820 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3101\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:55:02,841 [INFO ] W-9004-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3147\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:55:04,301 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Done! Transform dataset costs 0.65 seconds.\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:55:06,096 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3334\n",
      "\u001b[36malgo-1-jf0uq_1  |\u001b[0m 2020-01-15 00:55:06,096 [INFO ] W-9001-model ACCESS_LOG - /172.18.0.1:60884 \"POST /invocations HTTP/1.1\" 200 6125\n",
      "\n",
      "Prediction output: {'0': ['99.36% \\t Denver Broncos', '0.23% \\t The American Football Conference (AFC) champion Denver Broncos', '0.20% \\t Broncos'], '1': [\"25.86% \\t Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\", \"23.11% \\t Levi's Stadium\", '17.88% \\t San Francisco Bay Area at Santa Clara, California']}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = predictor.predict(my_test_examples)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction output: \n",
      "\n",
      "\n",
      "['99.36% \\t Denver Broncos', '0.23% \\t The American Football Conference (AFC) champion Denver Broncos', '0.20% \\t Broncos']\n",
      "\n",
      "\n",
      "[\"25.86% \\t Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\", \"23.11% \\t Levi's Stadium\", '17.88% \\t San Francisco Bay Area at Santa Clara, California']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrediction output: \\n\\n\")\n",
    "\n",
    "for k in output.keys():\n",
    "    print('{}\\n\\n'.format(output[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "\n",
    "Remove the endpoint after we are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "kernelspec": {"display_name": "conda_mxnet_p36","language": "python", "name": "conda_mxnet_p36"},"language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
